{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0eb552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegressionCV,LassoCV,ElasticNet,ElasticNetCV,RidgeCV,RidgeClassifierCV,ridge_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score,cross_val_predict,RepeatedStratifiedKFold,RepeatedKFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "#need \"pip install scikit-optimize\"\n",
    "from skopt.searchcv import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical \n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.tree import plot_tree\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbcec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions to perform graphing\n",
    "def graph_it(y_true,y_pred,title=\"Graph\",RQ=1):\n",
    "# do the different graphing\n",
    "    plt.rcParams.update({'font.sans-serif':'Arial'})\n",
    "    plt.figure(figsize=(7,7))\n",
    "\n",
    "    if (RQ == 1):\n",
    "        lables = np.array(False,True)\n",
    "    else: labels = np.array(['Delivered Early','Within 0-25%','Within 25-50%','Within 50-75%','Within 75-100%','Delivered Late'])\n",
    "    \n",
    "    #confusion matrix\n",
    "    cm = confusion_matrix(y_true,y_pred,labels=labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=labels)\n",
    "    disp.plot(cmap='Greys',colorbar=False)\n",
    "    plt.title(title)            \n",
    "    plt.show()\n",
    "\n",
    "def graph_feature(names,fi,graph_title,tree=True,add_label=True):\n",
    "    sort_key = fi.argsort()\n",
    "    plt.figure(figsize=(5,10))\n",
    "    bars = plt.barh(names[sort_key],fi[sort_key],color='lightgrey',edgecolor='black')\n",
    "    plt.title(graph_title)\n",
    "    \n",
    "    if (add_label==True and tree==True):\n",
    "        # Add annotation to top 5 bars\n",
    "        plt.xlabel('Feature Importance')        \n",
    "        full_count=len(bars)\n",
    "        exit_count=full_count\n",
    "        for bar in bars:\n",
    "            if(exit_count > 5):\n",
    "                exit_count = exit_count -1\n",
    "                continue\n",
    "            else:\n",
    "                width = bar.get_width()\n",
    "                label_y = bar.get_y() + bar.get_height() /4\n",
    "                plt.text(.01, label_y, s=f'{width:.4f}',fontweight='bold',color='black')\n",
    "                exit_count = exit_count - 1\n",
    "    elif (add_label==True and tree==False):\n",
    "        # Add annotation to top and bottom 3 bars\n",
    "        plt.xlabel('Coefficients') \n",
    "        full_count=len(bars)\n",
    "        exit_count=full_count\n",
    "        for bar in bars:\n",
    "            if(exit_count > 3 and exit_count <= full_count-3 ):\n",
    "                exit_count = exit_count -1\n",
    "                continue\n",
    "            else:\n",
    "                width = bar.get_width()\n",
    "                if (width > 0):\n",
    "                    plot_width = width-width+width/250\n",
    "                else:plot_width = width-width+width/1000\n",
    "                label_y = bar.get_y() + bar.get_height() /4\n",
    "                plt.text(plot_width, label_y, s=f'{width:.4f}',fontweight='bold',color='black')\n",
    "                exit_count = exit_count - 1    \n",
    "           \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the regression models\n",
    "def run_linear(X,Y,graph=False,graph_title='Regression Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'fit_intercept' : [True,False],\n",
    "        'positive' : [True,False]\n",
    "    }    \n",
    "    cv = KFold(n_splits = 10,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=LinearRegression(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=10,\n",
    "        #scoring=\"accuracy\",  -- leave as default which is based on the estimator\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    #scale the x predictor values and then run the Bayesian search and capture best parameters\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X)       \n",
    "    search.fit(x_scaled,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_scaled,Y,test_size=.24,random_state=5440)   \n",
    "\n",
    "    model = LinearRegression(n_jobs=-1,fit_intercept=best_params['fit_intercept'],positive=best_params['positive'])\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test))\n",
    "    r2_test = r2_score(y_test, pred_test)  \n",
    "    \n",
    "    if graph:\n",
    "        graph_feature(X.columns,model.coef_,graph_title,tree=False)\n",
    "    \n",
    "    return(rmse_test,r2_test)\n",
    "    \n",
    "# function for fitting trees of various depths for Random Forests\n",
    "def run_cross_validation_on_regression_RF(X, Y,graph=False,graph_title='Regression Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'n_estimators': [200, 400, 600, 800, 1000],\n",
    "        'max_depth': (1, 9),\n",
    "        'criterion': ['squared_error'], \n",
    "        'max_features' : [.250,.3333,.375]\n",
    "    }\n",
    "    cv = KFold(n_splits = 5,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=100,\n",
    "        #scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    \n",
    "    search.fit(X,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #now that the best parameters are found, split the data, run on a test dataset and then predict results\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=.24,random_state=5440)\n",
    "    model = RandomForestRegressor(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth']\n",
    "                                   ,criterion=best_params['criterion'],max_features=best_params['max_features'])\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)    \n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test))\n",
    "    r2_test = r2_score(y_test, pred_test)  \n",
    "    \n",
    "    if graph:\n",
    "        graph_feature(X.columns,model.feature_importances_,graph_title)\n",
    "    \n",
    "    return(rmse_test,r2_test)\n",
    "\n",
    "# function for fitting trees of various depths for Boosted Version\n",
    "def run_cross_validation_on_regression_Boost(X, Y,graph=False,graph_title='Regression Graph'):\n",
    "    #X = predictors, Y = response\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'n_estimators': [500, 600, 700, 800, 900, 1000],\n",
    "        'max_depth': (1, 9),\n",
    "        'criterion': ['friedman_mse','squared_error'],\n",
    "        'loss' : ['squared_error','huber'],\n",
    "        'max_features' : [.250,.3333,.375]\n",
    "    }    \n",
    "    cv = KFold(n_splits = 5,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=GradientBoostingRegressor(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=150,\n",
    "        #scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    \n",
    "    search.fit(X,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #now that the best parameters are found, split the data, run on a test dataset and then predict results\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=.24,random_state=5440)\n",
    "    model = GradientBoostingRegressor(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth']\n",
    "                                   ,criterion=best_params['criterion'],loss=best_params['loss'],max_features=best_params['max_features'])\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test))\n",
    "    r2_test = r2_score(y_test, pred_test)  \n",
    "    \n",
    "    if graph:\n",
    "        graph_feature(X.columns,model.feature_importances_,graph_title)\n",
    "    \n",
    "    return(rmse_test,r2_test)\n",
    "\n",
    "#enet regression:  handles E-Net and Lasso\n",
    "def run_enet_regression(X,Y,graph=False,graph_title='Regression Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'alpha' : [.0001,.0005,.001,.005,.01,.05,.1,.5,1.0,5,10,50,100,500],\n",
    "        'l1_ratio' : [.01,.05,.1,.3,.5,.7,.9,.95,.99,1],\n",
    "        'fit_intercept' : [True,False]\n",
    "    }    \n",
    "    cv = KFold(n_splits = 5,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=ElasticNet(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=200,\n",
    "        #scoring=\"accuracy\",  -- leave as default which is based on the estimator\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    #scale the x predictor values and then run the Bayesian search and capture best parameters\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X)       \n",
    "    search.fit(x_scaled,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_scaled,Y,test_size=.24,random_state=5440)   \n",
    "\n",
    "    model = ElasticNet(fit_intercept=best_params['fit_intercept'],alpha=best_params['alpha'],\n",
    "                         l1_ratio=best_params['l1_ratio'],random_state=5440)\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test))\n",
    "    r2_test = r2_score(y_test, pred_test)  \n",
    "    \n",
    "    if graph:\n",
    "        graph_feature(X.columns,model.coef_,graph_title,tree=False)\n",
    "    \n",
    "    return(rmse_test,r2_test)\n",
    "\n",
    "#ridge regression:  handles Ridge separately due to different hyperparameters and lack of feature selection\n",
    "def run_ridge_regression(X,Y,graph=False,graph_title='Regression Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "\n",
    "    #scale the x predictor values and then run the Bayesian search and capture best parameters\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X)       \n",
    "   \n",
    "    cv = RepeatedKFold(n_splits = 5,n_repeats=50,random_state=5440)  \n",
    "    model = RidgeCV(alphas=[.0001,.0005,.001,.005,.01,.05,.1,.5,1.0,5,10,50,100,500],cv=cv)\n",
    "    model.fit(x_scaled,Y)\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(model.alpha_,file=open('RQ3_hyperparameters','a'))\n",
    "\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_scaled,Y,test_size=.24,random_state=5440)   \n",
    "\n",
    "    pred_test = model.predict(x_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test,pred_test))\n",
    "    r2_test = r2_score(y_test, pred_test)  \n",
    "    \n",
    "    if graph:\n",
    "        graph_feature(X.columns,model.coef_,graph_title,tree=False)\n",
    "    \n",
    "    return(rmse_test,r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afd417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models used for classification\n",
    "def run_logistic(X,Y,graph=False,graph_title='Classification Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'fit_intercept' : [True,False],\n",
    "        'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    }    \n",
    "    cv = StratifiedKFold(n_splits = 10,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=LogisticRegressionCV(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=20,\n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    #scale the x predictor values and then run the Bayesian search and capture best parameters\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X)       \n",
    "    search.fit(x_scaled,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #now that the best parameters are found, split the data, run on a test dataset and then predict results\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_scaled,Y,test_size=.24,random_state=5440)\n",
    "    model = LogisticRegressionCV(cv=cv,fit_intercept=best_params['fit_intercept']\n",
    "                                 ,solver=best_params['solver'],scoring='accuracy',n_jobs=-1)\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    test_score = model.score(x_test,y_test)\n",
    "    test_auc = roc_auc_score(y_test,model.predict_proba(x_test), multi_class='ovr', average='weighted')      \n",
    "    class_groups = len(model.coef_)    \n",
    "      \n",
    "    if graph:\n",
    "        graph_it(y_test,pred_test,graph_title,RQ=3)\n",
    "        for cg in range(class_groups):\n",
    "            graph_feature(X.columns,model.coef_[cg],graph_title + ' (\"'+ model.classes_[cg]+ '\" class)',tree=False)\n",
    "\n",
    "    return(test_score,test_auc)\n",
    "\n",
    "# function for fitting trees of various depths using cross-validation\n",
    "def run_cross_validation_on_classification_RF(X, Y,graph=False,graph_title='Classification Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'n_estimators': [200, 400, 600, 800, 1000],\n",
    "        'max_depth': (1, 9),\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_features' : ['sqrt','log2']\n",
    "    }    \n",
    "    cv = StratifiedKFold(n_splits = 5,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=RandomForestClassifier(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=125,     \n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    \n",
    "    search.fit(X,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #now that the best parameters are found, split the data, run on a test dataset and then predict results\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=.24,random_state=5440)\n",
    "    model = RandomForestClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth']\n",
    "                                   ,criterion=best_params['criterion'],max_features=best_params['max_features'])\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    test_score = model.score(x_test,y_test)\n",
    "    test_auc = roc_auc_score(y_test,model.predict_proba(x_test), multi_class='ovr', average='weighted')     \n",
    "      \n",
    "    if graph:\n",
    "        graph_it(y_test,pred_test,graph_title,RQ=3)\n",
    "        graph_feature(X.columns,model.feature_importances_,graph_title)\n",
    "\n",
    "    return(test_score,test_auc)\n",
    "\n",
    "def run_cross_validation_on_classification_Boost(X, Y,scoring='accuracy',graph=False,graph_title='Classification Graph'):\n",
    "    #X = predictors, Y = response, log determines if we are using linear or logistic regression\n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search   \n",
    "    hyper_params = {\n",
    "        'n_estimators': [500, 750, 1000, 1250, 1500],\n",
    "        'max_depth': (1, 9),\n",
    "        'criterion': ['friedman_mse', 'squared_error'],\n",
    "        'max_features' : ['sqrt','log2']\n",
    "    }    \n",
    "    cv = StratifiedKFold(n_splits = 5,shuffle=True,random_state=5440)   #set random_state to make results repeatable\n",
    "    search = BayesSearchCV(\n",
    "        estimator=GradientBoostingClassifier(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=cv,\n",
    "        n_iter=125,\n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    \n",
    "    search.fit(X,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #now that the best parameters are found, split the data, run on a test dataset and then predict results\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=.24,random_state=5440)\n",
    "    model = GradientBoostingClassifier(n_estimators=best_params['n_estimators'],max_depth=best_params['max_depth']\n",
    "                                   ,criterion=best_params['criterion'],max_features=best_params['max_features'])\n",
    "    model.fit(x_train,y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    test_score = model.score(x_test,y_test)\n",
    "    test_auc = roc_auc_score(y_test,model.predict_proba(x_test), multi_class='ovr', average='weighted')     \n",
    "      \n",
    "    if graph:\n",
    "        graph_it(y_test,pred_test,graph_title,RQ=3)\n",
    "        graph_feature(X.columns,model.feature_importances_,graph_title)\n",
    "\n",
    "    return(test_score,test_auc)\n",
    "\n",
    "#RDA is Regularized Discriminant Analysis (similar to how elastic-net works with Lasso and Ridge)\n",
    "def run_RDA_classification(X,Y,graph=False,graph_title='Classification Graph'):\n",
    "    #X = predictors, Y = response, other numbers for the range of values\n",
    "    \n",
    "    #first step is to use a Bayes Search algorithm to find the optimal hyperparameters\n",
    "    #define hyperparameters to search\n",
    "    hyper_params = {\n",
    "        'solver' : ['lsqr'],\n",
    "        'shrinkage' : np.arange(0,1.01,.01)\n",
    "    }\n",
    "\n",
    "    search = BayesSearchCV(\n",
    "        estimator=LinearDiscriminantAnalysis(),\n",
    "        search_spaces=hyper_params,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        n_iter=100,\n",
    "        scoring=\"accuracy\",\n",
    "        verbose=0,\n",
    "        random_state=5440\n",
    "    )\n",
    "    \n",
    "    #find the hyperparameters on all the data and capture them for use for training and testing\n",
    "    search.fit(X,Y)    \n",
    "    best_params = search.best_params_\n",
    "    print(graph_title,file=open('RQ3_hyperparameters','a'))\n",
    "    print(best_params,file=open('RQ3_hyperparameters','a'))\n",
    "    \n",
    "    #scale the X values for consistency (though may not have much effect for LDA as it would knn, PCA, gradient decent and ridge/Lasso...)\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(X)\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x_scaled,Y,test_size=.24,random_state=5440)\n",
    "    model = LinearDiscriminantAnalysis(shrinkage=best_params['shrinkage'],solver=best_params['solver'])   \n",
    "    model.fit(x_train,y_train)\n",
    "     \n",
    "    #find the worth of the model  \n",
    "    pred_test = cross_val_predict(model,x_test,y_test,cv=5,n_jobs=-1)\n",
    "    pred_score = cross_val_score(model,x_test,y_test,cv=5,n_jobs=-1)\n",
    "    test_auc = roc_auc_score(y_test,model.predict_proba(x_test), multi_class='ovr', average='weighted')     \n",
    "    \n",
    "    class_groups = len(model.coef_)\n",
    "    \n",
    "    if graph:\n",
    "        graph_it(y_test,pred_test,graph_title,RQ=3)\n",
    "        for cg in range(class_groups):\n",
    "            graph_feature(X.columns,model.coef_[cg],graph_title + ' (\"'+ model.classes_[cg]+ '\" class)',tree=False)\n",
    "         \n",
    "    return(pred_score.mean(),test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa682ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other functions\n",
    "#function to handle multi-collinearity tests\n",
    "def vif_calc(X):\n",
    "    vif_info = pd.DataFrame()\n",
    "    vif_info['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif_info['Column'] = X.columns\n",
    "    vif_info.sort_values('VIF', ascending=False)\n",
    "    return(vif_info)\n",
    "\n",
    "#function to pass back AIC for linear model\n",
    "\n",
    "def aic_calc(X,Y):\n",
    "    #add constant to predictor variables\n",
    "    X = sm.add_constant(X)\n",
    "    #fit regression model\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    return(model.aic)\n",
    "\n",
    "#function to run baseline regression\n",
    "def get_stats(x,y,log=False):\n",
    "    if (log == True):\n",
    "        results = sm.Logit(y,x).fit()\n",
    "    else: results = sm.OLS(y,x).fit()\n",
    "    print(results.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105bc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the soybean dataset into a dataframe and confirm values\n",
    "full_start = timer()\n",
    "df_soy_raw = pd.read_csv('DataSets\\\\Soybean_Contracted_Deliveries_Export.csv')\n",
    "df_soy_raw.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the corn dataset into a dataframe and confirm the values\n",
    "df_corn_raw = pd.read_csv('DataSets\\\\Corn_Contracted_Deliveries_Export.csv')\n",
    "df_corn_raw.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d44503",
   "metadata": {},
   "source": [
    "Response variables are Diff_from_average_amt (regression) and Diff_from_average_category (classification)\n",
    "\n",
    "Prep both corn and soybean datasets and create both a \"full\" and \"partial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_soy = df_soy_raw.copy()\n",
    "df_corn = df_corn_raw.copy()\n",
    "\n",
    "#drop fields not needed for analysis\n",
    "df_soy = df_soy.drop(['commodity_name','net_freight_weight_qty','ship_to_range_val','days_from_deadline_val'],axis=1)\n",
    "df_corn = df_corn.drop(['commodity_name','net_freight_weight_qty','ship_to_range_val','days_from_deadline_val'],axis=1)\n",
    "\n",
    "#fill in null values with mean (only 21 out of 40K+)\n",
    "df_soy['DailyAverageSeaLevelPressure'] = df_soy['DailyAverageSeaLevelPressure'].fillna(df_soy['DailyAverageSeaLevelPressure'].mean())\n",
    "df_corn['DailyAverageSeaLevelPressure'] = df_corn['DailyAverageSeaLevelPressure'].fillna(df_corn['DailyAverageSeaLevelPressure'].mean())\n",
    "df_soy['DailyAverageStationPressure'] = df_soy['DailyAverageStationPressure'].fillna(df_soy['DailyAverageStationPressure'].mean())\n",
    "df_corn['DailyAverageStationPressure'] = df_corn['DailyAverageStationPressure'].fillna(df_corn['DailyAverageStationPressure'].mean())\n",
    "df_soy['DailyAverageRelativeHumidity'] = df_soy['DailyAverageRelativeHumidity'].fillna(df_soy['DailyAverageRelativeHumidity'].mean())\n",
    "df_corn['DailyAverageRelativeHumidity'] = df_corn['DailyAverageRelativeHumidity'].fillna(df_corn['DailyAverageRelativeHumidity'].mean())\n",
    "\n",
    "#remove boolean\n",
    "df_soy['is_midweek'] = df_soy['is_midweek'].astype(str)\n",
    "df_corn['is_midweek'] = df_corn['is_midweek'].astype(str)\n",
    "\n",
    "\n",
    "#create full data sets for each type of analysis\n",
    "ys_reg = df_soy['pct_elapsed_val']\n",
    "ys_class = df_soy['category_elapsed_val']\n",
    "xs_full = df_soy.drop(['pct_elapsed_val','category_elapsed_val'],axis=1)\n",
    "xs_full = pd.get_dummies(xs_full,drop_first = True) #make dummies for categorical values\n",
    "\n",
    "yc_reg = df_corn['pct_elapsed_val']\n",
    "yc_class = df_corn['category_elapsed_val']\n",
    "xc_full = df_corn.drop(['pct_elapsed_val','category_elapsed_val'],axis=1)\n",
    "xc_full = pd.get_dummies(xc_full,drop_first = True) #make dummies for categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc3ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look  at the vif calcuations for soybeans\n",
    "vif_calc(xs_full)\n",
    "#remove infinite VIF values\n",
    "xs_part = xs_full.drop(['max_price','prior_2_day_open_price','prior_day_open_price','DailyAverageStationPressure','DailyAverageRelativeHumidity','prior_2_day_open_diff','prior_3_day_open_diff'],axis=1)\n",
    "vif_calc(xs_part)\n",
    "\n",
    "xs_part = xs_part.drop(['recent_avg_price_diff','prior_day_open_diff','DailyAverageSeaLevelPressure','recent_avg_price','prior_3_day_open_price'],axis=1)\n",
    "vif_calc(xs_part)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90d743",
   "metadata": {},
   "source": [
    "Multi-colinearity is present, but not as bad as I've seen.  A bit more suprised at what is showing multicollinearity, since I don't have other price numeric fields not anything with relative humidity.  Do a check on the correlation of the ones that are heavily multi-collinear to see which would make sense to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do aic checks on which windspeed to remove\n",
    "aic_val =np.empty(4)\n",
    "aic_val[0] = aic_calc(xs_part,ys_reg)\n",
    "temp_x = xs_part.drop(['close_price','prior_3_day_open_price'],axis=1)\n",
    "aic_val[1] = aic_calc(temp_x,ys_reg)\n",
    "temp_x = xs_part.drop(['close_price','recent_avg_price'],axis=1)\n",
    "aic_val[2] = aic_calc(temp_x,ys_reg)\n",
    "temp_x = xs_part.drop(['prior_3_day_open_price','recent_avg_price'],axis=1)\n",
    "aic_val[3] = aic_calc(temp_x,ys_reg)\n",
    "print(aic_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same for corn\n",
    "vif_calc(xc_full)\n",
    "#remove infinite values\n",
    "\n",
    "xc_part = xc_full.drop(['max_price','prior_2_day_open_price','prior_day_open_price','DailyAverageStationPressure','DailyAverageRelativeHumidity','prior_2_day_open_diff','prior_3_day_open_diff'],axis=1)\n",
    "vif_calc(xc_part)\n",
    "\n",
    "xc_part = xc_part.drop(['recent_avg_price_diff','prior_day_open_diff','DailyAverageSeaLevelPressure','recent_avg_price','prior_3_day_open_price'],axis=1)\n",
    "vif_calc(xc_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589e8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soybeans Regression\n",
    "# With the regression functions defined, run the regressions and capture the RMSE and R-squared\n",
    "start = timer()\n",
    "linear_rmse,linear_r2 = run_linear(xs_part,ys_reg,graph=True,graph_title=\"Contract Soybeans - Linear Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Linear Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()      \n",
    "enet_full_rmse,enet_full_r2 = run_enet_regression(xs_full,ys_reg,graph=True,graph_title=\"Contract Soybeans - Lasso/ENet Regression - Full\")\n",
    "end = timer()\n",
    "print(f'Enet Regression Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()      \n",
    "enet_part_rmse,enet_part_r2 = run_enet_regression(xs_part,ys_reg,graph=True,graph_title=\"Contract Soybeans - Lasso/ENet Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Enet Regression Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "ridge_part_rmse,ridge_part_r2 = run_ridge_regression(xs_part,ys_reg,graph=True,graph_title=\"Contract Soybeans - Ridge Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Ridge Regression Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "rfr_rmse,rfr_r2 = run_cross_validation_on_regression_RF(xs_part,ys_reg,graph=True,graph_title=\"Contract Soybeans - Random Forests - Partial\")\n",
    "end = timer()\n",
    "print(f'Random Forest Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "boost_rmse,boost_r2 = run_cross_validation_on_regression_Boost(xs_full,ys_reg,graph=True,graph_title=\"Contract Soybeans - Boosted Trees - Full\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "boost_part_rmse,boost_part_r2 = run_cross_validation_on_regression_Boost(xs_part,ys_reg,graph=True,graph_title=\"Contract Soybeans - Boosted Trees - Partial\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Data Subset Complete in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee18fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soybeans Regression results\n",
    "result_ds_list  = [['Linear Run 1','Subset',linear_rmse,linear_r2]\n",
    "                  ,['ENet Run 1','Full',enet_full_rmse,enet_full_r2]\n",
    "                  ,['ENet Run 2','Subset',enet_part_rmse,enet_part_r2]                   \n",
    "                  ,['Ridge Run 1','Subset',ridge_part_rmse,ridge_part_r2]\n",
    "                  ,['Random Forest Run 1','Subset',rfr_rmse,rfr_r2]\n",
    "                  ,['Boosted Trees Run 1','Full',boost_rmse,boost_r2]\n",
    "                  ,['Boosted Trees Run 2','Subset',boost_part_rmse,boost_part_r2]]\n",
    "results_delivery_count = pd.DataFrame(result_ds_list,columns=['Model','Dataset','RMSE','R^2'])\n",
    "sort_results = results_delivery_count.sort_values(['R^2','RMSE'],ascending=[False,True])\n",
    "sort_results.to_excel('RQ3_Soybeans_Regression.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1835df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soybeans Classfication\n",
    "start = timer()\n",
    "log_accuracy_part,log_auc_part = run_logistic(xs_part,ys_class,graph=True,graph_title=\"Contract Soybeans - Logistic Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Logistic Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rda_accuracy_full,rda_auc_full = run_RDA_classification(xs_full,ys_class,graph=True,graph_title=\"Contract Soybeans - Discriminant Analysis - Full\")\n",
    "end = timer()\n",
    "print(f'RDA Model on Full Data Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rda_accuracy_part,rda_auc_part = run_RDA_classification(xs_part,ys_class,graph=True,graph_title=\"Contract Soybeans - Discriminant Analysis - Partial\")\n",
    "end = timer()\n",
    "print(f'RDA Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rf_accuracy_part,rf_auc_part = run_cross_validation_on_classification_RF(xs_part,ys_class,graph=True,graph_title=\"Contract Soybeans - Random Forests - Partial\")\n",
    "end = timer()\n",
    "print(f'Random Forest Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "boost_accuracy_full,boo_auc_full = run_cross_validation_on_classification_Boost(xs_full,ys_class,graph=True,graph_title=\"Contract Soybeans - Boosted Trees - Full\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "boost_accuracy_part,boo_auc_part = run_cross_validation_on_classification_Boost(xs_part,ys_class,graph=True,graph_title=\"Contract Soybeans - Boosted Trees - Partial\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Data Subset Complete in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3793e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soybeans Classification Results\n",
    "#create a data frame of the results for analysis\n",
    "result_aa_list  = [['Logistic Run 1','Partial',log_accuracy_part,log_uac_part]              \n",
    "                  ,['RDA Run 1','Full',rda_accuracy_full,rda_uac_full]\n",
    "                  ,['RDA Run 2','Partial',rda_accuracy_part,rda_uac_part]                \n",
    "                  ,['Random Forest Run 1','Partial',rf_accuracy_part,rf_uac_part]\n",
    "                  ,['Boosted Trees Run 1','Full',boost_accuracy_full,boo_uac_full]\n",
    "                  ,['Boosted Trees Run 2','Partial',boost_accuracy_part,boo_uac_part]]\n",
    "results_above_average = pd.DataFrame(result_aa_list,columns=['Model','Dataset','Accuracy','AUC'])\n",
    "sort_results = results_above_average.sort_values(['AUC','Accuracy'],ascending=[False,False])\n",
    "sort_results.to_excel('RQ3_Soybean_Classification.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e06d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corn Regression\n",
    "# With the regression functions defined, run the regressions and capture the RMSE and R-squared\n",
    "start = timer()\n",
    "linear_rmse,linear_r2 = run_linear(xc_part,yc_reg,graph=True,graph_title=\"Contract Corn - Linear Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Linear Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()      \n",
    "enet_full_rmse,enet_full_r2 = run_enet_regression(xc_full,yc_reg,graph=True,graph_title=\"Contract Corn - Lasso/ENet Regression - Full\")\n",
    "end = timer()\n",
    "print(f'Enet Regression Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()      \n",
    "enet_part_rmse,enet_part_r2 = run_enet_regression(xc_part,yc_reg,graph=True,graph_title=\"Contract Corn - Lasso/ENet Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Enet Regression Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "ridge_part_rmse,ridge_part_r2 = run_ridge_regression(xc_part,yc_reg,graph=True,graph_title=\"Contract Corn - Ridge Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Ridge Regression Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "rfr_rmse,rfr_r2 = run_cross_validation_on_regression_RF(xc_part,yc_reg,graph=True,graph_title=\"Contract Corn - Random Forests - Partial\")\n",
    "end = timer()\n",
    "print(f'Random Forest Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "boost_rmse,boost_r2 = run_cross_validation_on_regression_Boost(xc_full,yc_reg,graph=True,graph_title=\"Contract Corn - Boosted Trees - Full\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()  \n",
    "boost_part_rmse,boost_part_r2 = run_cross_validation_on_regression_Boost(xc_part,yc_reg,graph=True,graph_title=\"Contract Corn - Boosted Trees - Partial\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Data Subset Complete in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e27255",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corn Regression results\n",
    "c_result_ds_list  = [['Linear Run 1','Subset',linear_rmse,linear_r2]\n",
    "                  ,['ENet Run 1','Full',enet_full_rmse,enet_full_r2]\n",
    "                  ,['ENet Run 2','Subset',enet_part_rmse,enet_part_r2]                   \n",
    "                  ,['Ridge Run 1','Subset',ridge_part_rmse,ridge_part_r2]\n",
    "                  ,['Random Forest Run 1','Subset',rfr_rmse,rfr_r2]\n",
    "                  ,['Boosted Trees Run 1','Full',boost_rmse,boost_r2]\n",
    "                  ,['Boosted Trees Run 2','Subset',boost_part_rmse,boost_part_r2]]\n",
    "results_delivery_count = pd.DataFrame(c_result_ds_list,columns=['Model','Dataset','RMSE','R^2'])\n",
    "c_sort_results = results_delivery_count.sort_values(['R^2','RMSE'],ascending=[False,True])\n",
    "c_sort_results.to_excel('RQ3_Corn_Regression.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7837eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corn Classfication\n",
    "start = timer()\n",
    "log_accuracy_part,log_auc_part = run_logistic(xc_part,yc_class,graph=True,graph_title=\"Contract Corn - Logistic Regression - Partial\")\n",
    "end = timer()\n",
    "print(f'Logistic Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rda_accuracy_full,rda_auc_full = run_RDA_classification(xc_full,yc_class,graph=True,graph_title=\"Contract Corn - Discriminant Analysis - Full\")\n",
    "end = timer()\n",
    "print(f'RDA Model on Full Data Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rda_accuracy_part,rda_auc_part = run_RDA_classification(xc_part,yc_class,graph=True,graph_title=\"Contract Corn - Discriminant Analysis - Partial\")\n",
    "end = timer()\n",
    "print(f'RDA Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "rf_accuracy_part,rf_auc_part = run_cross_validation_on_classification_RF(xc_part,yc_class,graph=True,graph_title=\"Contract Corn - Random Forests - Partial\")\n",
    "end = timer()\n",
    "print(f'Random Forest Model on Data Subset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "boost_accuracy_full,boost_auc_full = run_cross_validation_on_classification_Boost(xc_full,yc_class,graph=True,graph_title=\"Contract Corn - Boosted Trees - Full\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Full Dataset Complete in {end-start} seconds')\n",
    "\n",
    "start = timer()\n",
    "boost_accuracy_part,boost_auc_part = run_cross_validation_on_classification_Boost(xc_part,yc_class,graph=True,graph_title=\"Contract Corn - Boosted Trees - Partial\")\n",
    "end = timer()\n",
    "print(f'Boosted Trees Model on Data Subset Complete in {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Soybeans Classification Results\n",
    "#create a data frame of the results for analysis\n",
    "c_result_aa_list  = [['Logistic Run 1','Partial',log_accuracy_part,log_auc_part]         \n",
    "                  ,['RDA Run 1','Full',rda_accuracy_full,rda_auc_full]\n",
    "                  ,['RDA Run 2','Partial',rda_accuracy_part,rda_auc_part]               \n",
    "                  ,['Random Forest Run 1','Partial',rf_accuracy_part,rf_auc_part]\n",
    "                  ,['Boosted Trees Run 1','Full',boost_accuracy_full,boost_auc_full]\n",
    "                  ,['Boosted Trees Run 2','Partial',boost_accuracy_part,boost_auc_part]]\n",
    "results_above_average = pd.DataFrame(c_result_aa_list,columns=['Model','Dataset','Accuracy','AUC'])\n",
    "c_sort_results = results_above_average.sort_values(['AUC','Accuracy'],ascending=[False,False])\n",
    "c_sort_results.to_excel('RQ3_Corn_Classification.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
